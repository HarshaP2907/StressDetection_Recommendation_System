{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4352556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process.py\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import math\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "from cv2 import VideoWriter_fourcc, VideoWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc426b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_brow_distance(leye,reye):\n",
    "    global points\n",
    "    distance = dist.euclidean(leye,reye)\n",
    "    #calculating euclidean distance between left eye and right eye\n",
    "    points.append(int(distance))\n",
    "    #adding distance to a list\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97901e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_finder(faces,frame):\n",
    "    global emotion_classifier\n",
    "    emotions_list = [\"angry\" ,\"disgust\",\"fear\", \"happy\", \"sad\", \"surprise\",\"neutral\"]\n",
    "    x,y,w,h = face_utils.rect_to_bb(faces)\n",
    "    frame = frame[y:y+h,x:x+w]\n",
    "    #region of interest - roi\n",
    "    roi = cv2.resize(frame,(64,64))\n",
    "    roi = roi.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi,axis=0)\n",
    "    preds = emotion_classifier.predict(roi)[0]\n",
    "    emotion_probability = np.max(preds)\n",
    "    label = emotions_list[preds.argmax()]\n",
    "    if label in ['fear','sad', 'neutral']:\n",
    "        label = 'stressed'\n",
    "    else:\n",
    "        label = 'not stressed'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa33ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(points,disp):\n",
    "    normalized_value = abs(disp - np.min(points))/abs(np.max(points) - np.min(points))\n",
    "    #max-min normalization\n",
    "    '''The distances between left and right eyebrows is normalized for each frame in the video and the stress value \n",
    "    is calculated using that normalized value '''\n",
    "    stress_value = np.exp(-(normalized_value))\n",
    "    return stress_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e3e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x000001D01FC50970>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp/ipykernel_4692/1942540570.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  normalized_value = abs(disp - np.min(points))/abs(np.max(points) - np.min(points))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9334352252850698\n",
      "0.3635805007669541\n",
      "0.3635805007669541\n",
      "0.3582875988499648\n",
      "0.44392036003607943\n",
      "0.3582875988499648\n",
      "0.43726467206980746\n",
      "0.3600942930281762\n",
      "0.36700475791317283\n",
      "0.42513462792741236\n",
      "0.42513462792741236\n",
      "0.600251518586586\n",
      "0.4179413128445052\n",
      "0.600251518586586\n",
      "0.600251518586586\n",
      "0.600251518586586\n",
      "0.3654039487624784\n",
      "0.36725866099476145\n",
      "0.46900725773563534\n",
      "0.6019572715378986\n",
      "0.4161390680694228\n",
      "0.6019572715378986\n",
      "0.4104102675478394\n",
      "0.5264957384941846\n",
      "0.5264957384941846\n",
      "0.5264957384941846\n",
      "0.5884916537362881\n",
      "0.4104102675478394\n",
      "0.5313434908333113\n",
      "0.46900725773563534\n",
      "0.6753395551030256\n",
      "0.46900725773563534\n",
      "0.5264957384941846\n",
      "0.46900725773563534\n",
      "0.5963005971398023\n",
      "0.5264957384941846\n",
      "0.6753395551030256\n",
      "0.5884916537362881\n",
      "0.6019572715378986\n",
      "0.6753395551030256\n",
      "0.6019572715378986\n",
      "0.7725518584743519\n",
      "0.7648290948039481\n",
      "0.86614464490287\n",
      "0.9808409782110764\n",
      "1.0\n",
      "0.48216231687059996\n",
      "0.9642616587435098\n",
      "0.48216231687059996\n",
      "0.48216231687059996\n",
      "0.49898400873493554\n",
      "0.46723777738325506\n",
      "0.4333656359304916\n",
      "0.4349932624861838\n",
      "0.451434119180767\n",
      "0.43616298845797197\n",
      "0.46590187454855775\n",
      "0.4349932624861838\n",
      "0.451434119180767\n",
      "0.4203105296700969\n",
      "0.40611992953359716\n",
      "0.46723777738325506\n",
      "0.4349932624861838\n",
      "0.41878486458729997\n",
      "0.48216231687059996\n",
      "0.45018469594369187\n",
      "0.45018469594369187\n",
      "0.46590187454855775\n",
      "0.4214066849263477\n",
      "0.4333656359304916\n",
      "0.48017454881679633\n",
      "0.4203105296700969\n",
      "0.40714806237983986\n",
      "0.4835922281019413\n",
      "0.451434119180767\n",
      "0.45018469594369187\n",
      "0.3791512303417027\n",
      "0.3924053697863211\n",
      "0.4333656359304916\n",
      "0.41878486458729997\n",
      "0.4168384671142832\n",
      "0.45018469594369187\n",
      "0.4349932624861838\n",
      "0.4349932624861838\n",
      "0.44844667978050157\n",
      "0.4333656359304916\n",
      "0.44844667978050157\n",
      "0.4046885855090244\n",
      "0.4640441489014569\n",
      "0.4214066849263477\n",
      "0.44844667978050157\n",
      "0.45018469594369187\n",
      "0.43616298845797197\n",
      "0.4349932624861838\n",
      "0.45018469594369187\n",
      "0.44844667978050157\n",
      "0.461676850036757\n",
      "0.4333656359304916\n",
      "0.4203105296700969\n",
      "0.4046885855090244\n",
      "0.4046885855090244\n",
      "0.4214066849263477\n",
      "0.4203105296700969\n",
      "0.40611992953359716\n",
      "0.41878486458729997\n",
      "0.40611992953359716\n",
      "0.44844667978050157\n",
      "0.40611992953359716\n",
      "0.4203105296700969\n",
      "0.43616298845797197\n",
      "0.43616298845797197\n",
      "0.4203105296700969\n",
      "0.451434119180767\n",
      "0.48216231687059996\n",
      "0.4203105296700969\n",
      "0.9661049965255964\n",
      "0.40611992953359716\n",
      "0.9978554838606931\n",
      "0.46590187454855775\n",
      "0.45018469594369187\n",
      "0.4203105296700969\n",
      "0.4203105296700969\n",
      "0.46590187454855775\n",
      "0.48216231687059996\n",
      "0.451434119180767\n",
      "0.43616298845797197\n",
      "0.43616298845797197\n",
      "0.4835922281019413\n",
      "0.4349932624861838\n",
      "0.46723777738325506\n",
      "0.8405170028338569\n",
      "0.4835922281019413\n",
      "0.5957163484843788\n",
      "0.46590187454855775\n",
      "0.5743061114689418\n",
      "0.48017454881679633\n",
      "0.5961628688829711\n",
      "0.45018469594369187\n",
      "0.6387284587719574\n",
      "0.46590187454855775\n",
      "0.5755424928145522\n",
      "0.4835922281019413\n",
      "0.6165955901952072\n",
      "0.43616298845797197\n",
      "0.6170787554426814\n",
      "0.48216231687059996\n",
      "0.5743061114689418\n",
      "0.46723777738325506\n",
      "0.5755424928145522\n",
      "0.451434119180767\n",
      "0.5957163484843788\n",
      "0.4835922281019413\n",
      "0.6387284587719574\n",
      "0.46590187454855775\n",
      "0.6165955901952072\n",
      "0.451434119180767\n",
      "0.5943813078599167\n",
      "0.43686770314365003\n",
      "0.6605683789791965\n",
      "0.4835922281019413\n",
      "0.5190210880508975\n",
      "0.5372173380049057\n",
      "0.5372173380049057\n",
      "0.5372173380049057\n",
      "0.5375735693311237\n",
      "0.5560504362963227\n",
      "0.5755424928145522\n",
      "0.5361516317907764\n",
      "0.5560504362963227\n",
      "0.5190210880508975\n",
      "0.5375735693311237\n",
      "0.5190210880508975\n",
      "0.5549034648808365\n",
      "0.5014402908763704\n",
      "0.5017490561548967\n",
      "0.5372173380049057\n",
      "0.5343853390186821\n",
      "0.5361516317907764\n",
      "0.5343853390186821\n",
      "0.5549034648808365\n",
      "0.5361516317907764\n",
      "0.5180294433064873\n",
      "0.48216231687059996\n",
      "0.5180294433064873\n",
      "0.5005163102032328\n",
      "0.49898400873493554\n",
      "0.46590187454855775\n",
      "0.49898400873493554\n",
      "0.5163853908209305\n",
      "0.49898400873493554\n",
      "0.5141017513074537\n",
      "0.5163853908209305\n",
      "0.5163853908209305\n",
      "0.5163853908209305\n",
      "0.5141017513074537\n",
      "0.5319330276840529\n",
      "0.5343853390186821\n",
      "0.5163853908209305\n",
      "0.4776426870136627\n",
      "0.48216231687059996\n",
      "0.49898400873493554\n",
      "0.5319330276840529\n",
      "0.49898400873493554\n",
      "0.5163853908209305\n",
      "0.5005163102032328\n",
      "0.5530031353721702\n",
      "0.4835922281019413\n",
      "0.5005163102032328\n",
      "0.49898400873493554\n",
      "0.5005163102032328\n",
      "0.5005163102032328\n",
      "0.5005163102032328\n",
      "0.5163853908209305\n",
      "0.5141017513074537\n",
      "0.4835922281019413\n",
      "0.48216231687059996\n",
      "0.48017454881679633\n",
      "0.4640441489014569\n",
      "0.49898400873493554\n",
      "0.49685471020103866\n",
      "0.5141017513074537\n",
      "0.5163853908209305\n",
      "0.49685471020103866\n",
      "0.5180294433064873\n",
      "0.5560504362963227\n",
      "0.5180294433064873\n",
      "0.5372173380049057\n",
      "0.5190210880508975\n",
      "0.5014402908763704\n",
      "0.451434119180767\n",
      "0.45018469594369187\n",
      "0.451434119180767\n",
      "0.45018469594369187\n",
      "0.45018469594369187\n",
      "0.46804293643895606\n",
      "0.4835922281019413\n",
      "0.46723777738325506\n",
      "0.4844542473970555\n",
      "0.451434119180767\n",
      "0.46723777738325506\n",
      "0.45218699579550187\n",
      "0.4214066849263477\n",
      "0.42206695699373603\n",
      "0.40797404404520005\n",
      "0.42206695699373603\n",
      "0.42206695699373603\n",
      "0.4214066849263477\n",
      "0.4835922281019413\n",
      "0.46723777738325506\n",
      "0.5005163102032328\n",
      "0.4835922281019413\n",
      "0.48216231687059996\n",
      "0.4835922281019413\n",
      "0.46723777738325506\n",
      "0.46723777738325506\n",
      "0.4214066849263477\n",
      "0.40611992953359716\n",
      "0.4349932624861838\n",
      "0.43616298845797197\n",
      "0.43686770314365003\n",
      "0.39395168933457236\n",
      "0.3924053697863211\n",
      "0.3924053697863211\n",
      "0.40714806237983986\n",
      "0.6340498469565101\n",
      "0.43616298845797197\n",
      "0.40611992953359716\n",
      "0.3933705168854874\n",
      "0.40611992953359716\n",
      "0.40714806237983986\n",
      "0.40714806237983986\n",
      "0.3933705168854874\n",
      "0.4214066849263477\n",
      "0.4214066849263477\n",
      "0.3933705168854874\n",
      "0.42206695699373603\n",
      "0.42206695699373603\n",
      "0.45018469594369187\n",
      "0.4203105296700969\n",
      "0.451434119180767\n",
      "0.9317550075894323\n",
      "0.46723777738325506\n",
      "0.9642616587435098\n",
      "0.4214066849263477\n",
      "0.451434119180767\n",
      "0.4203105296700969\n",
      "0.45218699579550187\n",
      "0.46804293643895606\n",
      "0.5014402908763704\n",
      "0.5180294433064873\n",
      "0.5180294433064873\n",
      "0.5005163102032328\n",
      "0.5372173380049057\n",
      "0.5361516317907764\n",
      "0.5755424928145522\n",
      "0.5549034648808365\n",
      "0.5372173380049057\n",
      "0.5361516317907764\n",
      "0.5343853390186821\n",
      "0.5530031353721702\n",
      "0.5163853908209305\n",
      "0.5005163102032328\n",
      "0.5163853908209305\n",
      "0.49898400873493554\n",
      "0.49685471020103866\n",
      "0.49685471020103866\n",
      "0.5141017513074537\n",
      "0.49685471020103866\n",
      "0.49685471020103866\n",
      "0.49685471020103866\n",
      "0.49685471020103866\n",
      "0.49685471020103866\n",
      "0.49898400873493554\n",
      "0.49898400873493554\n",
      "0.5163853908209305\n",
      "0.49898400873493554\n",
      "0.49685471020103866\n",
      "0.5163853908209305\n",
      "0.48216231687059996\n",
      "0.5141017513074537\n",
      "0.48216231687059996\n",
      "0.5163853908209305\n",
      "0.49685471020103866\n",
      "0.5141017513074537\n",
      "0.5163853908209305\n",
      "0.5180294433064873\n",
      "0.5163853908209305\n",
      "0.49898400873493554\n",
      "0.5343853390186821\n",
      "0.5005163102032328\n",
      "0.49898400873493554\n",
      "0.49898400873493554\n",
      "0.5163853908209305\n",
      "0.5319330276840529\n",
      "0.5530031353721702\n",
      "0.49898400873493554\n",
      "0.5549034648808365\n",
      "0.5943813078599167\n",
      "0.6151513359678512\n",
      "0.6151513359678512\n",
      "0.5943813078599167\n",
      "0.5549034648808365\n",
      "0.5190210880508975\n",
      "0.5190210880508975\n",
      "0.5193525113308978\n",
      "0.5180294433064873\n",
      "0.4844542473970555\n",
      "0.4844542473970555\n",
      "0.451434119180767\n",
      "0.43686770314365003\n",
      "0.42206695699373603\n",
      "0.45218699579550187\n",
      "0.46723777738325506\n",
      "0.44844667978050157\n",
      "0.4144823793406415\n",
      "0.4312898928723943\n",
      "0.4435513882946582\n",
      "END REACHED\n"
     ]
    }
   ],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "emotion_classifier = load_model(\"_mini_XCEPTION.102-0.66.hdf5\", compile=False)\n",
    "print(emotion_classifier, flush = True)\n",
    "input_video = cv2.VideoCapture('input_video.mp4')\n",
    "'''input_video = cv2.VideoCapture(0)\n",
    "fps = 30   #Frames per second\n",
    "size1= (int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))) \n",
    "#size --- this will capture the width and height of the video\n",
    "videoWriter = cv2.VideoWriter('MyVideo.avi', cv2.VideoWriter_fourcc('I','4','2','0'), fps, size1)\n",
    "\n",
    "success, frame = input_video.read()\n",
    "# read is a function not attribute since it is followed by ()\n",
    "# read fxn reads frame by frame\n",
    "# success is a boolean type variable\n",
    "\n",
    "numFramesRemaining = 10*fps - 1 \n",
    "# numFramesRemaining is the number of seconds video will get recorded\n",
    "\n",
    "while success and numFramesRemaining>0:\n",
    "    videoWriter.write(frame)\n",
    "    success, frame = input_video.read()\n",
    "    numFramesRemaining -= 1'''\n",
    "points = []\n",
    "stress_list = []\n",
    "stressval_list = []\n",
    "stressgraph = []\n",
    "size=0\n",
    "while(True):\n",
    "    _,frame = input_video.read()\n",
    "    if(not _): break\n",
    "    frame = cv2.flip(frame,1)\n",
    "    frame = imutils.resize(frame, width=500,height=500)\n",
    "    \n",
    "    (lBegin, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
    "    (rBegin, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "\n",
    "    #preprocessing the image\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    detections = detector(gray,0)\n",
    "    for detection in detections:\n",
    "        emotion = emotion_finder(detection,gray)\n",
    "        cv2.putText(frame, emotion, (10,10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        shape = predictor(frame,detection)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        leyebrow = shape[lBegin:lEnd]\n",
    "        reyebrow = shape[rBegin:rEnd]\n",
    "            \n",
    "        reyebrowhull = cv2.convexHull(reyebrow)\n",
    "        leyebrowhull = cv2.convexHull(leyebrow)\n",
    "\n",
    "        cv2.drawContours(frame, [reyebrowhull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [leyebrowhull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        distance = eye_brow_distance(leyebrow[-1],reyebrow[0])\n",
    "        stress_value = normalize_values(points,distance)\n",
    "        print(stress_value)\n",
    "        #if stress_value!=1.0: stress_list.append(stress_list)\n",
    "        if math.isnan(stress_value):\n",
    "            continue\n",
    "        cv2.putText(frame,\"stress level:{}\".format(str(int(stress_value*100))),(20,40),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        stress_list.append(frame)\n",
    "\n",
    "    height, width, layers = frame.shape\n",
    "    size = (width,height)\n",
    "    stressval_list.append(stress_value)\n",
    "out = cv2.VideoWriter('output_video.avi',cv2.VideoWriter_fourcc(*'DIVX'), 10, size)\n",
    "input_video.release()\n",
    "print(\"END REACHED\")\n",
    "for i in range(len(stress_list)):\n",
    "    out.write(stress_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c669e",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8922bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 588,327\n",
      "Trainable params: 587,175\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#train.py\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os\n",
    "\n",
    "num_classes = 7 #number of labels\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 32 #number of training examples utilized in one iteration\n",
    "\n",
    "train_data_dir = 'train' \n",
    "validation_data_dir = 'test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_rows,img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_rows,img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# LAYER 1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# LAYER 2\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# LAYER 3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# LAYER 4\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# LAYER 5\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# LAYER 6\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a98e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares__\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp/ipykernel_12520/2862243727.py:33: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 2.1704 - accuracy: 0.1925\n",
      "Epoch 00001: val_loss improved from inf to 1.78362, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 359s 468ms/step - loss: 2.1704 - accuracy: 0.1925 - val_loss: 1.7836 - val_accuracy: 0.2594 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.8319 - accuracy: 0.2306\n",
      "Epoch 00002: val_loss did not improve from 1.78362\n",
      "755/755 [==============================] - 354s 468ms/step - loss: 1.8319 - accuracy: 0.2306 - val_loss: 1.7851 - val_accuracy: 0.2571 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.7986 - accuracy: 0.2545\n",
      "Epoch 00003: val_loss improved from 1.78362 to 1.78306, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 336s 445ms/step - loss: 1.7986 - accuracy: 0.2545 - val_loss: 1.7831 - val_accuracy: 0.2567 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.7879 - accuracy: 0.2538\n",
      "Epoch 00004: val_loss did not improve from 1.78306\n",
      "755/755 [==============================] - 348s 461ms/step - loss: 1.7879 - accuracy: 0.2538 - val_loss: 1.7996 - val_accuracy: 0.2540 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.7810 - accuracy: 0.2571\n",
      "Epoch 00005: val_loss improved from 1.78306 to 1.72864, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 387s 512ms/step - loss: 1.7810 - accuracy: 0.2571 - val_loss: 1.7286 - val_accuracy: 0.2910 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.7537 - accuracy: 0.2773\n",
      "Epoch 00006: val_loss improved from 1.72864 to 1.72407, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 347s 459ms/step - loss: 1.7537 - accuracy: 0.2773 - val_loss: 1.7241 - val_accuracy: 0.3034 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.7287 - accuracy: 0.2928\n",
      "Epoch 00007: val_loss improved from 1.72407 to 1.68607, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 326s 431ms/step - loss: 1.7287 - accuracy: 0.2928 - val_loss: 1.6861 - val_accuracy: 0.3273 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.6836 - accuracy: 0.3138\n",
      "Epoch 00008: val_loss did not improve from 1.68607\n",
      "755/755 [==============================] - 346s 458ms/step - loss: 1.6836 - accuracy: 0.3138 - val_loss: 1.9314 - val_accuracy: 0.3132 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.6212 - accuracy: 0.3562\n",
      "Epoch 00009: val_loss improved from 1.68607 to 1.48651, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 362s 479ms/step - loss: 1.6212 - accuracy: 0.3562 - val_loss: 1.4865 - val_accuracy: 0.4267 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.5588 - accuracy: 0.3898\n",
      "Epoch 00010: val_loss did not improve from 1.48651\n",
      "755/755 [==============================] - 371s 492ms/step - loss: 1.5588 - accuracy: 0.3898 - val_loss: 1.4884 - val_accuracy: 0.4153 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.5268 - accuracy: 0.4024\n",
      "Epoch 00011: val_loss improved from 1.48651 to 1.35634, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 388s 515ms/step - loss: 1.5268 - accuracy: 0.4024 - val_loss: 1.3563 - val_accuracy: 0.4738 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4814 - accuracy: 0.4212\n",
      "Epoch 00012: val_loss did not improve from 1.35634\n",
      "755/755 [==============================] - 384s 508ms/step - loss: 1.4814 - accuracy: 0.4212 - val_loss: 1.3935 - val_accuracy: 0.4758 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4641 - accuracy: 0.4334\n",
      "Epoch 00013: val_loss improved from 1.35634 to 1.25112, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 374s 495ms/step - loss: 1.4641 - accuracy: 0.4334 - val_loss: 1.2511 - val_accuracy: 0.5262 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4516 - accuracy: 0.4402\n",
      "Epoch 00014: val_loss did not improve from 1.25112\n",
      "755/755 [==============================] - 409s 542ms/step - loss: 1.4516 - accuracy: 0.4402 - val_loss: 1.2859 - val_accuracy: 0.4980 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4360 - accuracy: 0.4486\n",
      "Epoch 00015: val_loss improved from 1.25112 to 1.19556, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 386s 512ms/step - loss: 1.4360 - accuracy: 0.4486 - val_loss: 1.1956 - val_accuracy: 0.5349 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4140 - accuracy: 0.4534\n",
      "Epoch 00016: val_loss did not improve from 1.19556\n",
      "755/755 [==============================] - 355s 471ms/step - loss: 1.4140 - accuracy: 0.4534 - val_loss: 1.2026 - val_accuracy: 0.5296 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4042 - accuracy: 0.4613\n",
      "Epoch 00017: val_loss improved from 1.19556 to 1.16403, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 362s 480ms/step - loss: 1.4042 - accuracy: 0.4613 - val_loss: 1.1640 - val_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4024 - accuracy: 0.4627\n",
      "Epoch 00018: val_loss did not improve from 1.16403\n",
      "755/755 [==============================] - 361s 479ms/step - loss: 1.4024 - accuracy: 0.4627 - val_loss: 1.1782 - val_accuracy: 0.5444 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3879 - accuracy: 0.4698\n",
      "Epoch 00019: val_loss improved from 1.16403 to 1.14554, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 350s 463ms/step - loss: 1.3879 - accuracy: 0.4698 - val_loss: 1.1455 - val_accuracy: 0.5615 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3763 - accuracy: 0.4734\n",
      "Epoch 00020: val_loss improved from 1.14554 to 1.13325, saving model to Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 332s 440ms/step - loss: 1.3763 - accuracy: 0.4734 - val_loss: 1.1333 - val_accuracy: 0.5507 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3743 - accuracy: 0.4712\n",
      "Epoch 00021: val_loss did not improve from 1.13325\n",
      "755/755 [==============================] - 327s 433ms/step - loss: 1.3743 - accuracy: 0.4712 - val_loss: 1.1487 - val_accuracy: 0.5524 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3641 - accuracy: 0.4793\n",
      "Epoch 00022: val_loss did not improve from 1.13325\n",
      "755/755 [==============================] - 320s 424ms/step - loss: 1.3641 - accuracy: 0.4793 - val_loss: 1.2194 - val_accuracy: 0.5423 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3501 - accuracy: 0.4824Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.13325\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "755/755 [==============================] - 331s 439ms/step - loss: 1.3501 - accuracy: 0.4824 - val_loss: 1.2107 - val_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint('Users\\hp\\BIOMETRICS\\Stress_Detector\\Emotion_little_vgg.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3, #epoch 1, 2, 3 if not improved, it will stop running\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "epochs=25\n",
    "\n",
    "history = model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)\n",
    "\n",
    "model.save('trained_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fef371e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0df6480985ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy and Loss Graph'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upper right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Accuracy and Loss Graph', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb663f",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e985020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "angry : 6.643951\n",
      "disgust : 0.619694\n",
      "fear : 7.966957\n",
      "happy : 53.667473\n",
      "sad : 4.416847\n",
      "surprise : 1.6760803\n",
      "neutral : 25.008991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "#test.py\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "# loading files\n",
    "haar_file=\"haarcascade_frontalface_default (1).xml\"\n",
    "emotion_model='_mini_XCEPTION.102-0.66.hdf5'\n",
    "\n",
    "cascade=cv2.CascadeClassifier(haar_file)\n",
    "emotion_classifier=load_model(emotion_model,compile=True)\n",
    "emotion_names=[\"angry\",\"disgust\",\"fear\", \"happy\", \"sad\", \"surprise\",\"neutral\"]\n",
    "frame=cv2.imread('yy.jpeg')\n",
    "gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "faces=cascade.detectMultiScale(gray_frame,1.5,5)\n",
    "text=[]\n",
    "for (x,y,w,h) in faces:\n",
    "    roi=gray_frame[y:y+h,x:x+w]\n",
    "    roi=cv2.resize(roi,(64,64))\n",
    "    roi=roi.astype(\"float\")/255.0\n",
    "    roi=img_to_array(roi)\n",
    "    roi=np.expand_dims(roi,axis=0)\n",
    "    \n",
    "    predicted_emotion=emotion_classifier.predict(roi)[0]\n",
    "    probab=np.max(predicted_emotion)\n",
    "    label=emotion_names[predicted_emotion.argmax()]\n",
    "    percen=predicted_emotion*100\n",
    "    for j in range(7):\n",
    "        text.append(emotion_names[j]+\" : \"+str(percen[j]))\n",
    "    for i in range(7):    \n",
    "        #cv2.putText(frame,text[i],(5,i*30+15),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,255),2)\n",
    "        print(text[i])\n",
    "    cv2.putText(frame,label,(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "cv2.imwrite('yy__result.jpeg', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f9b07dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are  sad\n",
      "['Do things you enjoy (or used to)', 'Get quality exercise', 'Challenge negative thinking', 'Get a nutritious diet', 'List what you like about yourself']\n",
      "https://www.vandrevalafoundation.com/\n",
      "https://www.healthline.com/health/mental-health/why-am-i-sad-for-no-reason\n",
      "https://www.youtube.com/watch?v=P6aPg3YBvBQ\n"
     ]
    }
   ],
   "source": [
    "print(\"You are \", label)\n",
    "tips = {\"fear\":[\"Drink Water\",\"Get a good night's sleep\",\"Eat wholesome meals\",\"Go for a walk\",\"Turn off news feed/social media\",\"Talk to someone\"],\n",
    "       \"angry\":[\"Repeat gentle phrases to yourself\",\"Take a walk\",\"Use virtualization to calm down\",\"Focus on your breathing\",\"Phone a friend\",\"Watch a standup comedy\"],\n",
    "       \"sad\":[\"Do things you enjoy (or used to)\",\"Get quality exercise\",\"Challenge negative thinking\",\"Get a nutritious diet\",\"List what you like about yourself\"]}\n",
    "\n",
    "\n",
    "website_links = {\"fear\":[\"https://www.businessinsider.in/science/health/heres-how-to-take-care-of-yourself-if-youre-feeling-scared-or-sad-right-now/articleshow/55342883.cms\",\n",
    "                \"https://mhanational.org/what-can-i-do-when-im-afraid\"],\n",
    "        \"angry\":[\"https://www.thehotline.org/resources/how-to-cool-off-when-youre-angry/\",\n",
    "                \"https://www.verywellmind.com/5-things-to-do-if-you-feel-angry-5092021\"],\n",
    "        \"sad\":[\"https://www.vandrevalafoundation.com/\",\"https://www.healthline.com/health/mental-health/why-am-i-sad-for-no-reason\"]}\n",
    "\n",
    "\n",
    "youtube_links = {\"fear\":[\"https://www.youtube.com/watch?v=IAODG6KaNBc\"],\n",
    "                \"angry\":[\"https://www.youtube.com/watch?v=P6aPg3YBvBQ\"],\n",
    "                \"sad\":[\"https://www.youtube.com/watch?v=P6aPg3YBvBQ\"]}\n",
    "\n",
    "\n",
    "song_links = {\"fear\":[],\n",
    "             \"angry\":[],\n",
    "             \"sad\":[]}\n",
    "\n",
    "\n",
    "if (label=='happy' or label=='neutral' or label=='surprised'):\n",
    "    #songs\n",
    "    print(\"hi\")\n",
    "    \n",
    "elif (label=='angry' or label=='sad'):\n",
    "    print(tips.get(label))\n",
    "    for i in website_links.get(label):\n",
    "        print(i)\n",
    "    for j in youtube_links.get(label):\n",
    "        print(j)\n",
    "    #songs\n",
    "    #tips\n",
    "    #resources\n",
    "    \n",
    "elif (label=='fear'):\n",
    "    #kmkm\n",
    "    print(tips.get('fear'))\n",
    "    print(website_links.get('fear'))\n",
    "    print(youtube_links.get('fear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b9d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d287f1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('https://www.vandrevalafoundation.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0844e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in website_links.get('sad'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ad5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some song suggestions for your mood:\n",
      "https://www.youtube.com/watch?v=vGZhMIXH62M\n",
      "https://www.youtube.com/watch?v=Pkh8UtuejGw\n"
     ]
    }
   ],
   "source": [
    "from playsound import playsound\n",
    "\n",
    "labels = [\"happy\", \"angry\", \"fear\", \"disgust\", \"sad\", \"surprised\", \"neutral\"]\n",
    "label = \"happy\"\n",
    "tips = {\"fear\":[\"Drink water\",\"Get a good night's sleep\",\"Eat wholesome meals\",\"Go for a walk\",\"Turn off news feed/social media\",\"Talk to someone\"],\n",
    "        \"angry\":[\"Repeat gentle phrases to yourself\",\"Take a walk\",\"Use visualization to calm down\",\"Focus on your breathing\",\"Phone a friend\",\"Watch a stand up comedy\"],\n",
    "        \"sad\":[\"Do things you enjoy (or used to)\",\"Get quality exercise\",\"Eat a nutritious diet\",\"Challenge negative thinking\"]\n",
    "       }\n",
    "website_links = {\"fear\":[\"https://www.businessinsider.in/science/health/heres-how-to-take-care-of-yourself-if-youre-feeling-scared-or-sad-right-now/articleshow/55342883.cms\",\"https://mhanational.org/what-can-i-do-when-im-afraid\"],\n",
    "                 \"angry\":[\"https://www.thehotline.org/resources/how-to-cool-off-when-youre-angry/\",\"https://www.mayoclinic.org/healthy-lifestyle/adult-health/in-depth/anger-management/art-20045434\"],\n",
    "                 \"sad\":[\"https://www.vandrevalafoundation.com/\",\"https://www.healthline.com/health/depression/recognizing-symptoms#fatigue\"]\n",
    "                }\n",
    "youtube_links = {\"fear\":[\"https://www.youtube.com/watch?v=IAODG6KaNBc\"],\n",
    "                 \"angry\":[\"https://www.youtube.com/watch?v=P6aPg3YBvBQ\"],\n",
    "                 \"sad\":[\"https://www.youtube.com/watch?v=P6aPg3YBvBQ\"]\n",
    "                }\n",
    "song_links = {\"fear\":[\"https://www.youtube.com/watch?v=GyA8ccqwp-4&feature=youtu.be\",\"https://www.bing.com/videos/search?q=alone+part+2&docid=607990227673701963&mid=1B6860319511BF2C5CC21B6860319511BF2C5CC2&view=detail&FORM=VIRE\"],\n",
    "              \"angry\":[\"https://www.youtube.com/watch?v=e74wLJ_KRes&feature=youtu.be\",\"https://www.youtube.com/watch?v=JwWz-94a_Hk&feature=youtu.be\"],\n",
    "              \"sad\":[\"https://www.youtube.com/watch?v=25ROFXjoaAU&feature=youtu.be\",\"https://www.youtube.com/watch?v=BzE1mX4Px0I\"],\n",
    "              \"happy\":[\"https://www.youtube.com/watch?v=vGZhMIXH62M\",\"https://www.youtube.com/watch?v=Pkh8UtuejGw\"]\n",
    "             }\n",
    "tunes = {\"fear\":'fear.mp3',\n",
    "         \"angry\":'angry.mp3',\n",
    "         \"sad\":'sad.mp3'\n",
    "         }\n",
    "\n",
    "if (label == \"happy\"):\n",
    "    # songs\n",
    "    print(\"Here are some song suggestions for your mood:\")\n",
    "    for s in song_links.get('happy'):\n",
    "        print(s)\n",
    "\n",
    "elif (label == \"angry\"):\n",
    "    # songs\n",
    "    print(\"Here are some song suggestions for your mood:\")\n",
    "    for s in song_links.get('angry'):\n",
    "        print(s)\n",
    "    # tips\n",
    "    print(\"Here are some tips to help you feel better:\")\n",
    "    for i in tips.get('angry'):\n",
    "        print(\"-> \"+i)\n",
    "    # resources\n",
    "    print(\"Here are some resources that you may find beneficial:\")\n",
    "    for j in website_links.get('angry'):\n",
    "        print(j)\n",
    "    for k in youtube_links.get('angry'):\n",
    "        print(k)\n",
    "    # tunes\n",
    "    print(\"Here's a tune that will help you calm down.\")\n",
    "    playsound(tunes.get('angry'))\n",
    "\n",
    "elif (label == \"fear\"):\n",
    "    # songs\n",
    "    print(\"Here are some song suggestions for your mood:\")\n",
    "    for s in song_links.get('fear'):\n",
    "        print(s)\n",
    "    # tips\n",
    "    print(\"Here are some tips to help you feel better:\")\n",
    "    for i in tips.get('fear'):\n",
    "        print(\"-> \"+i)\n",
    "    # resources\n",
    "    print(\"Here are some resources that you may find beneficial:\")\n",
    "    for j in website_links.get('fear'):\n",
    "        print(j)\n",
    "    for k in youtube_links.get('fear'):\n",
    "        print(k)\n",
    "    # tunes\n",
    "    print(\"Here's a tune that will make you feel better.\")\n",
    "    playsound(tunes.get('fear'))\n",
    "\n",
    "elif (label == \"sad\"):\n",
    "    # songs\n",
    "    print(\"Here are some song suggestions for your mood:\")\n",
    "    for s in song_links.get('sad'):\n",
    "        print(s)\n",
    "    # tips\n",
    "    print(\"Here are some tips to help you feel better:\")\n",
    "    for i in tips.get('sad'):\n",
    "        print(\"-> \"+i)\n",
    "    # resources\n",
    "    print(\"Here are some resources that you may find beneficial:\")\n",
    "    for j in website_links.get('sad'):\n",
    "        print(j)\n",
    "    for k in youtube_links.get('sad'):\n",
    "        print(k)\n",
    "    # tunes\n",
    "    print(\"Listen to a tune that will soothe you.\")\n",
    "    playsound(tunes.get('sad'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357d01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
